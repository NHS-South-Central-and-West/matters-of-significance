[
  {
    "objectID": "index.html#why-should-we-care",
    "href": "index.html#why-should-we-care",
    "title": "Matters of Significance",
    "section": "Why Should We Care?",
    "text": "Why Should We Care?\n\nP-values and “statistical significance” are widespread in statistics and analytics–but they are controversial.\nIf you have been exposed to these ideas before, you have probably been taught a way of doing things that is now widely discredited.\nAddressing these issues is difficult because the consensus among experts hasn’t filtered through to the teaching."
  },
  {
    "objectID": "index.html#what-is-a-p-value",
    "href": "index.html#what-is-a-p-value",
    "title": "Matters of Significance",
    "section": "What is a P-Value?",
    "text": "What is a P-Value?\n\nA p-value is “the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (Wasserstein and Lazar 2016).\nThis definition is technically correct, but what does it really mean? This question is the source of great debate (Schervish 1996; Aschwanden 2015; Gelman 2016; Greenland et al. 2016; Wasserstein and Lazar 2016)!\nPut simply (but not technically correctly), p-values are a measure of how surprising the observed data would be if the null hypothesis (that no effect exists in the population) is true."
  },
  {
    "objectID": "index.html#regression-example",
    "href": "index.html#regression-example",
    "title": "Matters of Significance",
    "section": "Regression Example",
    "text": "Regression Example\n\n\n# set random seed\nnp.random.seed(42)\n\n# sample size\nN = 200\n\n# generate predictors + noise\nx1, x2, x3, noise = [np.random.normal(size=N) for _ in range(4)]\n\n# outcome with specified effects\ny = 1 + 0.1 * x1 + 0.5 * x2 + 0.01 * x3 + noise\n\n# create dataframe\ndf = pd.DataFrame(dict(y=y, x1=x1, x2=x2, x3=x3))\n\n# fit ols model\nX = sm.add_constant(df[['x1', 'x2', 'x3']])\nlm_results = sm.OLS(df.y, X).fit()\n\n# render regression table\ncreate_regression_table(lm_results)\n\n\n\n\nTable 1: Linear Regression Results (N = 200))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome = y\n\n\nEstimate\n(95% CI)\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n1.03***\n[0.89, 1.18]\n0.07\n14.29\n0.000\n\n\nx1\n0.2*\n[0.05, 0.35]\n0.08\n2.55\n0.011\n\n\nx2\n0.39***\n[0.25, 0.53]\n0.07\n5.34\n0.000\n\n\nx3\n0.13\n[-0.02, 0.27]\n0.07\n1.75\n0.081\n\n\n\n* p&lt;0.05; ** p&lt;0.01; *** p&lt;0.001\n\n\nR2=0.167; Adj. R2=0.155"
  },
  {
    "objectID": "index.html#it-is-all-this-guys-fault",
    "href": "index.html#it-is-all-this-guys-fault",
    "title": "Matters of Significance",
    "section": "It is All This Guy’s Fault",
    "text": "It is All This Guy’s Fault\n\nRonald A. Fisher"
  },
  {
    "objectID": "index.html#and-a-little-bit-these-guys",
    "href": "index.html#and-a-little-bit-these-guys",
    "title": "Matters of Significance",
    "section": "And a Little Bit These Guys",
    "text": "And a Little Bit These Guys\n\n\n\n\n\nJerzy Neyman\n\n\n\n\n\n\n\nEgon Pearson"
  },
  {
    "objectID": "index.html#and-these-two-didnt-help",
    "href": "index.html#and-these-two-didnt-help",
    "title": "Matters of Significance",
    "section": "And These Two Didn’t Help",
    "text": "And These Two Didn’t Help\n\n\n\n\n\nPierre Simon Laplace\n\n\n\n\n\n\n\nKarl Pearson"
  },
  {
    "objectID": "index.html#a-brief-history-of-p-values",
    "href": "index.html#a-brief-history-of-p-values",
    "title": "Matters of Significance",
    "section": "A Brief History of P-Values",
    "text": "A Brief History of P-Values\n\nEarly uses of significance testing date to the 1700s (Laplace, Arbuthnot), with modern form emerging ~1900s (Pearson, Gosset).\nRonald A. Fisher (1936) formalized p-value in Statistical Methods for Research Workers, suggesting p=0.05 as a convenient cutoff.\nNeyman-Pearson (1933a; 1933b) introduced formal hypothesis testing with pre-chosen \\(\\alpha\\) (0.05).\nFisher later regretted the use of a rigid threshold like 0.05, but it remains ingrained in practice.\nTakeaway: p-values grew from Fisher’s flexible idea into an oft-rigid threshold, setting the stage for current debates."
  },
  {
    "objectID": "index.html#common-misinterpretations-misuses",
    "href": "index.html#common-misinterpretations-misuses",
    "title": "Matters of Significance",
    "section": "Common Misinterpretations & Misuses",
    "text": "Common Misinterpretations & Misuses\n\nP-values and “statistical significance” are often misused. Sometimes deliberately.\nThey are often used as proof the effect is real or substantively important.\nThey are often treated as a measure of the strength of the effect, and as the most important piece of evidence in an anylsis.\nWe rarely acknowledge that it can vary from sample to sample and that multiple comparisons can lead to false positives.\nP-hacking and other such practices can deliberately mislead.\nStatistical significance encourages a flawed way of thinking.\nAnd many, many more (Greenland et al. 2016)."
  },
  {
    "objectID": "index.html#simulate-p-value-distributions",
    "href": "index.html#simulate-p-value-distributions",
    "title": "Matters of Significance",
    "section": "Simulate P-Value Distributions",
    "text": "Simulate P-Value Distributions\n\nSimulating 1,000 t-tests on zero effect (null) and 0.2 effect (alt), iteratively increasing sample size.\nHow should we expect p-values for the null and alt to be distributed? And how will sample size influence the distribution?\n\n\nnp.random.seed(42) # set random seed\nn_sims = 1000 # number of simulations\neffects = [0.0, 0.2] # simulated effect sizes\nsample_sizes = [10, 20, 50, 100, 250, 500, 1000] # simulated sample sizes\n\n# simulate p-values\np_values = simulate_pvals(effects, sample_sizes, n_sims)"
  },
  {
    "objectID": "index.html#p-value-distributions-n50",
    "href": "index.html#p-value-distributions-n50",
    "title": "Matters of Significance",
    "section": "P-Value Distributions (n=50)",
    "text": "P-Value Distributions (n=50)\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nsns.histplot(\n    data=p_values.filter(pl.col(\"N\") == 50).to_pandas(),\n    x=\"p\", hue=\"effect\", palette=colors,\n    alpha=0.7,  bins=20, edgecolor=\".3\"\n)\n\nax.axvline(x=0.05, color='#D93649', linestyle='--', linewidth=5)\n\nax.get_legend().set_title(\"Effect Size\")\nplt.ylabel(\"Simulations\")\nplt.xlabel(\"P-Value\")\n\nplt.xlim(0, 1)\nplt.tight_layout()\nplt.show()\n\nreport_metrics(p_values, n=50)\n\n\n\n\n\nFigure 1: n = 50 per group, 1000 simulations\n\n\n\n\n\n\n\n\nSimulation Results:\n- Sample Size Per Group: 50\n- Simulations Per Condition: 1000\n- False Positive Rate (H₀): 0.05 (Expected: 0.05)\n- Statistical Power (H₁): 0.18 (Target: 0.8)\n- Mean p-value (H₀ vs H₁): 0.50 vs 0.35"
  },
  {
    "objectID": "index.html#p-value-distributions-n100",
    "href": "index.html#p-value-distributions-n100",
    "title": "Matters of Significance",
    "section": "P-Value Distributions (n=100)",
    "text": "P-Value Distributions (n=100)\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nsns.histplot(\n    data=p_values.filter(pl.col(\"N\") == 100).to_pandas(),\n    x=\"p\", hue=\"effect\", palette=colors,\n    alpha=0.8,  bins=20, edgecolor=\".3\"\n)\n\nax.axvline(x=0.05, color='#D93649', linestyle='--', linewidth=5)\n\nax.get_legend().set_title(\"Effect Size\")\nplt.ylabel(\"Simulations\")\nplt.xlabel(\"P-Value\")\n\nplt.xlim(0, 1)\nplt.tight_layout()\nplt.show()\n\nreport_metrics(p_values, n=100)\n\n\n\n\n\nFigure 2: n = 100 per group, 1000 simulations\n\n\n\n\n\n\n\n\nSimulation Results:\n- Sample Size Per Group: 100\n- Simulations Per Condition: 1000\n- False Positive Rate (H₀): 0.05 (Expected: 0.05)\n- Statistical Power (H₁): 0.30 (Target: 0.8)\n- Mean p-value (H₀ vs H₁): 0.50 vs 0.29"
  },
  {
    "objectID": "index.html#p-value-distributions-n500",
    "href": "index.html#p-value-distributions-n500",
    "title": "Matters of Significance",
    "section": "P-Value Distributions (n=500)",
    "text": "P-Value Distributions (n=500)\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nsns.histplot(\n    data=p_values.filter(pl.col(\"N\") == 500).to_pandas(),\n    x=\"p\", hue=\"effect\", palette=colors,\n    alpha=0.7,  bins=20, edgecolor=\"black\"\n)\n\nplt.axvline(x=0.05, color='#D93649', linestyle='--', linewidth=5)\n\nax.get_legend().set_title(\"Effect Size\")\nplt.ylabel(\"Simulations\")\nplt.xlabel(\"P-Value\")\n\nplt.xlim(0, 1)\nplt.tight_layout()\nplt.show()\n\nreport_metrics(p_values, n=500)\n\n\n\n\n\nFigure 3: n = 500 per group, 1000 simulations\n\n\n\n\n\n\n\n\nSimulation Results:\n- Sample Size Per Group: 500\n- Simulations Per Condition: 1000\n- False Positive Rate (H₀): 0.05 (Expected: 0.05)\n- Statistical Power (H₁): 0.89 (Target: 0.8)\n- Mean p-value (H₀ vs H₁): 0.51 vs 0.03"
  },
  {
    "objectID": "index.html#simulate-multiple-comparisons",
    "href": "index.html#simulate-multiple-comparisons",
    "title": "Matters of Significance",
    "section": "Simulate Multiple Comparisons",
    "text": "Simulate Multiple Comparisons\n\nSimulating running 50 tests where there is zero effect.\nRunning the simulation 1,000 times.\nHow often would we observe a “statistically significant” effect in at least one test?\n\n\nnp.random.seed(42) # set random seed\nn_sims = 1000 # number of simulations\nn_tests = 50 # number of tests per simulation\nalpha = 0.05 # significance threshold\n\n# simulate multiple comparisons\nmultiple_comparisons = simulate_multiple_comparisons(n_tests, n_sims, alpha)"
  },
  {
    "objectID": "index.html#multiple-comparison-problems",
    "href": "index.html#multiple-comparison-problems",
    "title": "Matters of Significance",
    "section": "Multiple Comparison Problems",
    "text": "Multiple Comparison Problems\n\n\nexpected = alpha * n_tests\nfig, ax = plt.subplots(figsize=(8, 6))\n\nsns.histplot(\n    data=multiple_comparisons.to_pandas(), x=\"false_positives\",\n    bins=range(0, 12), color=\"#005EB8\", edgecolor=\"black\"\n)\n\nplt.axvline(x=expected, color='#D93649', linestyle='--', linewidth=5)\n\nplt.xlabel(\"False Positives (p &lt; 0.05)\")\nplt.xticks(range(0, 11, 1))\nplt.legend([f\"Expected ({n_tests} × {alpha} = {expected})\"])\n\nplt.tight_layout()\nplt.show()\n\nreport_multiple_comparisons(multiple_comparisons, n_tests, n_sims, alpha)\n\n\n\n\n\nFigure 4: 1000 simulations × 50 tests, no true effects\n\n\n\n\n\n\n\n\nSimulation Results:\n- Average number of false positives: 2.51\n- Probability of at least one false positive: 92.8%\n- Expected number of false positives: 2.5"
  },
  {
    "objectID": "index.html#focus-on-measuring-effect-sizes",
    "href": "index.html#focus-on-measuring-effect-sizes",
    "title": "Matters of Significance",
    "section": "Focus on Measuring Effect Sizes",
    "text": "Focus on Measuring Effect Sizes\n\nAny analysis should focus on the magnitude and direction of the effect above all else.\nThe effect estimates are the part of the model that matters, and everything else, including p-values, is there to help measure model fit and identify potential issues.\nThis approach is rooted in the idea that the size of the effect is the bit that is directly grounded in the model’s context.\nA p-value doesn’t tell us anything about the outcome we are studying. The effect size does."
  },
  {
    "objectID": "index.html#diagnostic-or-descriptive-p-values",
    "href": "index.html#diagnostic-or-descriptive-p-values",
    "title": "Matters of Significance",
    "section": "Diagnostic (or Descriptive) P-Values",
    "text": "Diagnostic (or Descriptive) P-Values\n\nP-values do not tell you whether the results of your analysis are real, correct, or substantively important. But I don’t think p-values should be dismissed entirely.\nInstead, we should use p-values as a tool for checking if we have enough data to observe the effect we are studying.\n\nWe are capable of asking questions and designing analyses where the effect will not be zero (Gelman, Hill, and Vehtari 2021).\nIf we start by assuming there is an effect, p-values tell us whether we have enough data to observe that effect.\n\nAdditionally, I think there is still value in using p-values as a quick check to identify whether two groups are different, or other similar descriptive questions, but p-values themselves are not a causal measure."
  },
  {
    "objectID": "index.html#same-effect-different-sample-sizes",
    "href": "index.html#same-effect-different-sample-sizes",
    "title": "Matters of Significance",
    "section": "Same Effect, Different Sample Sizes",
    "text": "Same Effect, Different Sample Sizes\n\n\nsummary = (\n    p_values\n    .filter(pl.col(\"effect\") == \"0.2\")\n    .with_columns((pl.col(\"p\") &gt; 0.05).alias(\"non_sig\"))\n    .group_by(\"N\")\n    .agg([\n        pl.col(\"non_sig\").sum().alias(\"non_sig_count\"),\n        pl.len().alias(\"total\")\n    ])\n    .with_columns(\n        (pl.col(\"non_sig_count\") /\n        pl.col(\"total\")).alias(\"rejection_rate\")\n    )\n    .select([\"N\", \"non_sig_count\", \"rejection_rate\"])\n    .sort(\"N\")\n)\n\nformat_power_table(summary)\n\n\n\n\nTable 2: Proportion of Non-Significant Results (p &gt; 0.05)\n\n\n\n\n\n\n\n\n\nSample Size\nRejections\nProportion\n\n\n\n\n10\n925\n0.93\n\n\n20\n908\n0.91\n\n\n50\n817\n0.82\n\n\n100\n701\n0.70\n\n\n250\n392\n0.39\n\n\n500\n113\n0.11\n\n\n1000\n5\n0.01\n\n\n\nEffect = 0.2, 1000 Simulations"
  },
  {
    "objectID": "index.html#scrap-statistical-significance",
    "href": "index.html#scrap-statistical-significance",
    "title": "Matters of Significance",
    "section": "Scrap Statistical Significance",
    "text": "Scrap Statistical Significance\n\nWhile I am less of an evangelist when it comes to p-values, I won’t say the same for statistical significance.\nUsing a cutoff point for deciding whether an analysis is useful or not is bad practice.\n“The difference between ‘significant’ and ‘not significant’ is not itself statistically significant” (Gelman and Stern 2006)."
  },
  {
    "objectID": "index.html#key-takeaways",
    "href": "index.html#key-takeaways",
    "title": "Matters of Significance",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nP-values are a useful tool, but we need to give them significantly less weight in the analysis process.\nWe should scrap statistical significance and focus on measuring effect sizes.\nP-values and statistical significance was only ever a cheap shortcut, but doing science (or analytics) well is hard. We shouldn’t take shortcuts."
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "Matters of Significance",
    "section": "Further Reading",
    "text": "Further Reading\n\nDenworth - The Significant Problem of P-Values\n538’s Science Coverage:\n\nAschwanden - Not Even Scientists Can Easily Explain P-values\nAschwanden - It’s Time To Stop Misusing P-Values\nAschwanden - Science Isn’t Broken\n\nGelman & Loken – The Garden of Forking Paths\nMcElreath – Statistical Rethinking\nThe ASA Statement on P-Values\nGigerenzer – Mindless Statistics"
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Matters of Significance",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nHack Your Way to Scientific Glory\nVisualisations (of Key Concepts in Statistics)"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Matters of Significance",
    "section": "References",
    "text": "References\n\n\n\nAschwanden, Christie. 2015. “Not Even Scientists Can Easily Explain p-Values.” FiveThirtyEight.com.\n\n\nFisher, Ronald Aylmer. 1936. “Statistical Methods for Research Workers.”\n\n\nGelman, Andrew. 2016. “The Problems with p-Values Are Not Just with p-Values.” The American Statistician 70 (10): 1–2.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Cambridge University Press.\n\n\nGelman, Andrew, and Hal Stern. 2006. “The Difference Between ‘Significant’ and ‘Not Significant’ Is Not Itself Statistically Significant.” The American Statistician 60 (4): 328–31.\n\n\nGreenland, Sander, Stephen J Senn, Kenneth J Rothman, John B Carlin, Charles Poole, Steven N Goodman, and Douglas G Altman. 2016. “Statistical Tests, p Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50.\n\n\nNeyman, Jerzy, and Egon S Pearson. 1933a. “The Testing of Statistical Hypotheses in Relation to Probabilities a Priori.” In Mathematical Proceedings of the Cambridge Philosophical Society, 29:492–510. 4. Cambridge University Press.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933b. “On the Problem of the Most Efficient Tests of Statistical Hypotheses.” Philosophical Transactions of the Royal Society of London 231 (694-706): 289–337.\n\n\nSchervish, Mark J. 1996. “P Values: What They Are and What They Are Not.” The American Statistician 50 (3): 203–6.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician. Taylor & Francis."
  }
]